---
title: "SériesTemporellesTP1"
author: "Lélio"
date: "2024-01-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list=ls())
```
## Exercice 1
```{r}
# Chargement du jeu de données
ozone = read.table(file = "Dataset_ozone.txt", sep = ";", header = TRUE, dec = ",")
```
### Question 1
```{r}
# Attribution des variables réponse et explicatives
Y = ozone$maxO3
X = ozone$T12

#Calcul des moyennes empiriques
X_bar = mean(X)
Y_bar = mean(Y)

#Calcul de beta1 et beta0
beta1_manuel <- sum((X - X_bar) * (Y - Y_bar)) / sum((X - X_bar)^2)
beta0_manuel <- Y_bar - beta1_manuel * X_bar

#Calcul du modèle de régression linéaire
y = beta0_manuel + beta1_manuel*X
plot(X, Y, main = "Régression linéaire entre la valeur maximale d'ozone et la température relevée à 12H", ylab = "Max03 : valeur maximale d'ozone par jour", xlab = "T12 : Température relevée à 12H")
lines(X, y, col = 'red', lwd = 2)
legend("topright", legend = "Droite de régression", col = "red", lwd = 2)
```

### Question 2
En utilisant $lm$
```{r}
#Calcul de la régression linéaire à l'aide de la commande lm
prediction = lm(Y ~ X, data = ozone)

#On récupère les coefficients beta0 et beta1 contenus dans la liste que génère la commande lm
beta0_lm = coef(prediction)[1]
beta1_lm = coef(prediction)[2]

#Calcul du modèle de régression linéaire avec les coefficients obtenus avec lm
y_lm = beta0_lm + beta1_lm*X
plot(X, Y, main = "Régression linéaire entre la valeur maximale d'ozone et la température relevée à 12H", ylab = "Max03 : valeur maximale d'ozone par jour", xlab = "T12 : Température relevée à 12H")
lines(X, y_lm, col = 'green', lwd = 2)
legend("topright", legend = "Droite de régression", col = "green", lwd = 2)


```

On obtient bien en effet les mêmes résultats en utilisant lm

### Question 3
Pour évaluer la qualité du modèle, on utilise la formule du cours qui permet de calculer $R^2$
$$R^2 = \frac{||\hat{Y}-\overline{Y}_n||^2}{||Y-\overline{Y}_n||^2} = \frac{\sum(\hat{Y}_i-\overline{Y}_n)^2}{\sum(Y_i-\overline{Y}_n)^2}$$
```{r}
# Calcul des estimateurs
beta_1barre = sum((X - X_bar) * (Y - Y_bar)) / sum((X - X_bar)^2)
beta_0barre = Y_bar - beta_1barre*X_bar

#Calcul de \hat{Y}
hatY = beta_0barre + beta_1barre*X

#Calcul du numérateur et dénominateur
SSE = sum((hatY-Y_bar)^2)
SST = sum((Y-Y_bar)^2)

#Calcul de R2
R2_manuel = SSE/SST
cat("R2 : ", R2_manuel)
```

### Avec lm

```{r}
prediction = lm(Y~X, data = ozone)
R2_lm = summary.lm(prediction)$r.squared
cat("R2 : ", R2_lm)
```
$R^2$ calculé avec la formule du cours semble être plus précis qu'avec lm

## Question 4
On souhaite calculer les bornes inferieurs et supérieurs de l'intervalle de confiance.
On utilise la formule vue en cours 
$$\beta_0 +\beta_1X_{n+1} \in \bigg[\hat{\beta}_0 + \hat{\beta}_1 \pm t_{1-\frac{\alpha}{2};n-2}\cdot \hat\sigma_n\sqrt{1 + \frac{1}{n}+ \frac{(x_{n+1}-\overline{x}_n)^2}{\sum(x_i-\overline{x}_n)^2}}\bigg] $$
```{r}
n = length(Y)
# On définit alpha = 0.05 pour avoir un intervalle de confiance à 95%
alpha = 0.05

#Fonction qt pour le quantile d'une loi de Student a n-2 degrés de liberté
t = qt(1-alpha/2, n-2)

#Calcul du \hat{\sigma}
hat_sigma = sqrt(sum((Y-beta0_manuel - beta1_manuel*X)^2)/(n-2))

#Calcul de l'intervalle de confiance
interv = t*hat_sigma *sqrt(1 + (1/n) + (X-mean(X))/(sum((X-mean(X))^2)))
borneInf = beta_0barre + beta_1barre*X - interv
borneSup = beta_0barre + beta_1barre*X + interv

#Représentation graphique
plot(X,Y, main = "Intervalles de confiance pour Y à 95%", ylab = "Max03 : valeur maximale d'ozone par jour", xlab = "T12 : Température relevée à 12H")
lines(X, y_lm, col = 'yellow', lwd = 2)
curve(beta_0barre + beta_1barre*x - t*hat_sigma*sqrt(1 + (1/n) + (x-mean(X))/(sum((x-mean(X))^2))), from=min(X), to=max(X), col="blue", add=TRUE, lwd = 2)
curve(beta_0barre + beta_1barre*x + t*hat_sigma*sqrt(1 + (1/n) + (x-mean(X))/(sum((x-mean(X))^2))), from=min(X), to=max(X), col="purple", add=TRUE, lwd = 2)
legend("topright", legend = c("Borne supérieur","Droite de régression", "Borne inférieur"), col = c("purple", "yellow", "blue"), lwd = 2)
```
Commentaire : à faire
## Question 5
```{r}
prediction = lm(Y~X, data = ozone)
summary(prediction)
```
La commande $lm$ donne plusieurs informations sur la régression linéaire entre $X$ et $Y$ comme par exemple l'ordonnée à l'origine appelée intercept et la pente appelée $X$, toutes deux trouvables dans la section coefficients.
On peut aussi obtenir les résidus studentisés dans la section residuals.

## Question 6
Hypothèse de bruit gaussien.
$$Y_i = \beta_0 + \beta_1X_i + \varepsilon_i$$
$$\hat{\varepsilon}_i= Y_i - \hat{Y}_i$$
```{r}
par(mfrow = c(2,2))
plot(prediction)
shapiro.test(prediction$residuals)
```
Le test de Shapiro-Wilk permet de vérifier si les résidus suivent une loi normale.
Ici, on obtient $W = 0.99235$ et $p-value = 0.792$.
Le fait que $W$ soit proche de $1$ signifie que les résidus ne présentent pas de déviation par rapport à la normalité.
La valeur de $p-value$ étant bien au dessus de $0.05$ le seuil usuel, on en conclut qu'il n'y a pas assez de preuves pour rejeter la normalité des résidus.
En conclusion, ce test permet de dire que les résidus semblent suivre une loi normale


## Exercice 2
```{r}
rm(list=ls())
```
On considère à présent les données contenues dans data1.RData.
On souhaite étudier la dépendance de la consommation électrique (kwh) avec des variables
décrivant la température (cldd et htdd).
## Question 1
```{r}
load("data1.RData")
data = khct.df
```
## Question 2
```{r}
pairs(data)
```

## Question 3
D'accord.

## Question 4
Régression linéaire à l'aide de la fonction $lm$ en considérant jusqu'a 1983.
```{r}
data = data[1:168,]
Y = sqrt(data$kwh)
X1 = data$htdd
X2 = data$cldd
prediction = lm(Y ~ X1 + X2, data = data)

#On récupère les coefficients beta0 et beta1 contenus dans la liste que génère la commande lm
beta0_lm = coef(prediction)[1]
beta1_lm = coef(prediction)[2]
beta2_lm = coef(prediction)[3]

#Calcul du modèle de régression linéaire avec les coefficients obtenus avec lm
y_lm = beta0_lm + beta1_lm*X1 + beta2_lm*X2

plot(data$t1, Y, main = "Régression linéaire multiple la racine du kwh et cldd+htdd", ylab = "KWH", xlab = "Temps en années")
lines(data$t1, y_lm, col = 'red', lwd = 3)
legend("topright", legend = "Régression multiple", col = "red", lwd = 3)

```
## Question 5
Le modèle ne semble pas correspondre, les hypothèses de bruit ne sont pas satisfaites.

## Question 6
On considère le nouveau modèle liant $\sqrt{kwh}$, $htdd$, $cldd$, $t1$ et $t2=(t1-1977)^2$.

```{r}
data = data[1:168,]
Y = sqrt(data$kwh)
X1 = data$htdd
X2 = data$cldd
X3 = data$t1
X4 = (data$t1-1977)^2

prediction = lm(Y ~ X1 + X2 + X3 + X4, data = data)

#On récupère les coefficients beta0 et beta1 contenus dans la liste que génère la commande lm
beta0_lm = coef(prediction)[1]
beta1_lm = coef(prediction)[2]
beta2_lm = coef(prediction)[3]
beta3_lm = coef(prediction)[4]
beta4_lm = coef(prediction)[5]

#Calcul du modèle de régression linéaire avec les coefficients obtenus avec lm
y_lm = beta0_lm + beta1_lm*X1 + beta2_lm*X2 + beta3_lm*X3 + beta4_lm*X4
plot(data$t1, Y, main = "Régression linéaire multiple la racine du kwh et cldd+htdd+t1+t2=(t1-1977)²", xlab = "Temps en années", ylab = "KWH")
lines(data$t1, y_lm, col = 'lightgreen', lwd = 3)
legend("topright", legend = "Régression multiple", col = "lightgreen", lwd = 3)
```
## Question 7
```{r}
rm(list=ls())
load("data1.RData")
data = khct.df
data = data[169:180,]

Y = sqrt(data$kwh)
X1 = data$htdd
X2 = data$cldd
X3 = data$t1
X4 = (data$t1-1977)^2

prediction = lm(Y ~ X1 + X2 + X3 + X4, data = data)

#On récupère les coefficients beta0 et beta1 contenus dans la liste que génère la commande lm
beta0_lm = coef(prediction)[1]
beta1_lm = coef(prediction)[2]
beta2_lm = coef(prediction)[3]
beta3_lm = coef(prediction)[4]
beta4_lm = coef(prediction)[5]

#Calcul du modèle de régression linéaire avec les coefficients obtenus avec lm
y_lm = beta0_lm + beta1_lm*X1 + beta2_lm*X2 + beta3_lm*X3 + beta4_lm*X4
plot(data$t1, Y, main = "Régression linéaire multiple la racine du kwh et cldd+htdd+t1+t2=(t1-1977)²", xlab = "Temps en mois pour l'année 1984", ylab = "KWH")
lines(data$t1, y_lm, col = 'lightblue', lwd = 3)
legend("topright", legend = "Régression multiple", col = "lightblue", lwd = 3)
```
Hypothèse de normalité du bruit : 
$$\varepsilon \sim \mathcal{N}(0,\sigma^2I\alpha)$$
$$\hat\varepsilon_{n+1}\sim\mathcal{N}(0,\overbrace{\sigma^2(X_{n+1}(\mathbb{X}^t\mathbb{X})^{-1}X_{n+1}+1}^{m^2}))$$
$$V(\hat\varepsilon_{n+1}) = V(X_{n+1}(\beta-\hat\beta)) + \underbrace{V(\varepsilon_{n+1})}_{\sigma^2}$$
$$\varepsilon_{n+1}\perp \!\!\! \perp \beta$$
Intervalle de confiance : 
On cherche $r$ tel que 
$$P(Y_{n+1} \in [\hat Y_{n+1}\pm r]) = 1-\alpha$$
$$P\bigg(\frac{X_{n+1} - \hat Y_{n+1}}{\sigma m} \in \bigg[\pm \frac{r}{\sigma m}\bigg]\bigg) = 1 - \alpha$$
$$ P\bigg(\frac{|\hat\varepsilon_{n+1}|}{\hat\sigma_n m} \leq t_{n-rk(\mathbb{X})}(1-\frac{\alpha}{2})\bigg) = 1 - \alpha$$
Intervalle de confiance à $100(1-\alpha)\%$ : 
$$X_{n+1} \in \bigg[ \hat Y_{n+1}\pm \hat\sigma_n\cdot t_{n-rk(\mathbb{X})}(1-\frac{\alpha}{2})\sqrt{1 + X_{n+1}(\mathbb{X}^t\mathbb{X})^{-1}X_{n+1}}\bigg] $$
où $n-rk(\mathbb{X})$ est le nombre de degrés de liberté de la loi de Student.
$X_k = (1, X_k^{(1)}, ..., X_k^{(p)})$.
Ici, $p=4$, $X^{(3)} = t_1$.
On va chercher à tracer $Y_k$ en fonction de $t_1$ et de même pour $\hat Y_k$.

```{r}
# Nombre d'observations
n = length(Y)

# On définit alpha = 0.2 pour avoir un intervalle de confiance à 80%
alpha = 0.2

# Degré de liberté 
d = n - length(coef(prediction))

# Quantile de la distribution t
t = qt(1 - alpha / 2, d)

# \hat{\sigma}
hat_sigma = sqrt(sum((Y - y_lm)^2) / (n - 2))

# Calcul des intervalles de confiance
interv = t * hat_sigma * sqrt(1 +
              (1/n) +
              ((X1 - mean(X1))^2 / sum((X1 - mean(X1))^2)) +
              ((X2 - mean(X2))^2 / sum((X2 - mean(X2))^2)) +
              ((X3 - mean(X3))^2 / sum((X3 - mean(X3))^2)) +
              ((X4 - mean(X4))^2 / sum((X4 - mean(X4))^2)))

borneInf = y_lm - interv
borneSup = y_lm + interv

# Tracé du graphique avec l'intervalle de confiance
plot(data$t1, Y, main = "Régression linéaire multiple la racine du kwh et cldd+htdd+t1+t2=(t1-1977)²", xlab = "Temps en mois pour l'année 1984", ylab = "KWH")
lines(data$t1, y_lm, col = 'lightblue', lwd = 3)
lines(data$t1, borneSup, col = 'red', lty = 2)
lines(data$t1, borneInf, col = 'red', lty = 2)
legend("topright", legend = c("Borne supérieure","Régression multiple", "Borne inférieure"), col = c("red", "lightblue", "red"), lwd = c(2, 3, 2), lty = c(2, 1, 2))
```

