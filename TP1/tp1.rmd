---
title: "SériesTemporellesTP1"
author: "Lélio"
date: "2024-01-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list=ls())
```
## Exercice 1
```{r}
# Chargement du jeu de données
ozone = read.table(file = "Dataset_ozone.txt", sep = ";", header = TRUE, dec = ",")
```
### Question 1
```{r}
# Attribution des variables réponse et explicatives
Y = ozone$maxO3
X = ozone$T12

#Calcul des moyennes empiriques
X_bar = mean(X)
Y_bar = mean(Y)

#Calcul de beta1 et beta0
beta1_manuel <- sum((X - X_bar) * (Y - Y_bar)) / sum((X - X_bar)^2)
beta0_manuel <- Y_bar - beta1_manuel * X_bar

#Calcul du modèle de régression linéaire
y = beta0_manuel + beta1_manuel*X
plot(X, Y, main = "Régression linéaire entre la valeur maximale d'ozone et la température relevée à 12H", ylab = "Max03 : valeur maximale d'ozone par jour", xlab = "T12 : Température relevée à 12H")
lines(X, y, col = 'red', lwd = 2)
legend("topright", legend = "Droite de régression", col = "red", lwd = 2)
```

### Question 2
En utilisant $lm$
```{r}
#Calcul de la régression linéaire à l'aide de la commande lm
prediction = lm(Y ~ X, data = ozone)

#On récupère les coefficients beta0 et beta1 contenus dans la liste que génère la commande lm
beta0_lm = coef(prediction)[1]
beta1_lm = coef(prediction)[2]

#Calcul du modèle de régression linéaire avec les coefficients obtenus avec lm
y_lm = beta0_lm + beta1_lm*X
plot(X, Y, main = "Régression linéaire entre la valeur maximale d'ozone et la température relevée à 12H", ylab = "Max03 : valeur maximale d'ozone par jour", xlab = "T12 : Température relevée à 12H")
lines(X, y_lm, col = 'green', lwd = 2)
legend("topright", legend = "Droite de régression", col = "green", lwd = 2)


```

On obtient bien en effet les mêmes résultats en utilisant lm

### Question 3
Pour évaluer la qualité du modèle, on utilise la formule du cours qui permet de calculer $R^2$
$$R^2 = \frac{||\hat{Y}-\overline{Y}_n||^2}{||Y-\overline{Y}_n||^2} = \frac{\sum(\hat{Y}_i-\overline{Y}_n)^2}{\sum(Y_i-\overline{Y}_n)^2}$$
```{r}
# Calcul des estimateurs
beta_1barre = sum((X - X_bar) * (Y - Y_bar)) / sum((X - X_bar)^2)
beta_0barre = Y_bar - beta_1barre*X_bar

#Calcul de \hat{Y}
hatY = beta_0barre + beta_1barre*X

#Calcul du numérateur et dénominateur
SSE = sum((hatY-Y_bar)^2)
SST = sum((Y-Y_bar)^2)

#Calcul de R2
R2_manuel = SSE/SST
cat("R2 : ", R2_manuel)
```

### Avec lm

```{r}
prediction = lm(Y~X, data = ozone)
R2_lm = summary.lm(prediction)$r.squared
cat("R2 : ", R2_lm)
```
$R^2$ calculé avec la formule du cours semble être plus précis qu'avec lm

## Question 4
On souhaite calculer les bornes inferieurs et supérieurs de l'intervalle de confiance.
On utilise la formule vue en cours 
$$\beta_0 +\beta_1X_{n+1} \in \bigg[\hat{\beta}_0 + \hat{\beta}_1 \pm t_{1-\frac{\alpha}{2};n-2}\cdot \hat\sigma_n\sqrt{1 + \frac{1}{n}+ \frac{(x_{n+1}-\overline{x}_n)^2}{\sum(x_i-\overline{x}_n)^2}}\bigg] $$
```{r}
n = length(Y)
# On définit alpha = 0.05 pour avoir un intervalle de confiance à 95%
alpha = 0.05

#Fonction qt pour le quantile d'une loi de Student a n-2 degrés de liberté
t = qt(1-alpha/2, n-2)

#Calcul du \hat{\sigma}
hat_sigma = sqrt(sum((Y-beta0_manuel - beta1_manuel*X)^2)/(n-2))

#Calcul de l'intervalle de confiance
interv = t*hat_sigma *sqrt(1 + (1/n) + (X-mean(X))/(sum((X-mean(X))^2)))
borneInf = beta_0barre + beta_1barre*X - interv
borneSup = beta_0barre + beta_1barre*X + interv

#Représentation graphique
plot(X,Y, main = "Intervalles de confiance pour Y à 95%", ylab = "Max03 : valeur maximale d'ozone par jour", xlab = "T12 : Température relevée à 12H")
lines(X, y_lm, col = 'yellow', lwd = 2)
curve(beta_0barre + beta_1barre*x - t*hat_sigma*sqrt(1 + (1/n) + (x-mean(X))/(sum((x-mean(X))^2))), from=min(X), to=max(X), col="blue", add=TRUE, lwd = 2)
curve(beta_0barre + beta_1barre*x + t*hat_sigma*sqrt(1 + (1/n) + (x-mean(X))/(sum((x-mean(X))^2))), from=min(X), to=max(X), col="purple", add=TRUE, lwd = 2)
legend("topright", legend = c("Borne supérieur","Droite de régression", "Borne inférieur"), col = c("purple", "yellow", "blue"), lwd = 2)
```
Commentaire : à faire
## Question 5
```{r}
prediction = lm(Y~X, data = ozone)
summary(prediction)
```
La commande $lm$ donne plusieurs informations sur la régression linéaire entre $X$ et $Y$ comme par exemple l'ordonnée à l'origine appelée intercept et la pente appelée $X$, toutes deux trouvables dans la section coefficients.
On peut aussi obtenir les résidus studentisés dans la section residuals.

## Question 6
Hypothèse de bruit gaussien.
$$Y_i = \beta_0 + \beta_1X_i + \varepsilon_i$$
$$\hat{\varepsilon}_i= Y_i - \hat{Y}_i$$
$\Longrightarrow$ résidus studentisés, vérifier qu'ils suivent une loi de Student ($n-3$ degrés de liberté)
```{r}
par(mfrow = c(2,2))
plot(prediction)
shapiro.test(prediction$residuals)
```
Le test de Shapiro-Wilk permet de vérifier si les résidus suivent une loi normale.
Ici, on obtient $W = 0.99235$ et $p-value = 0.792$.
Le fait que $W$ soit proche de $1$ signifie que les résidus ne présentent pas de déviation par rapport à la normalité.
La valeur de $p-value$ étant bien au dessus de $0.05$ le seuil usuel, on en conclut qu'il n'y a pas assez de preuves pour rejeter la normalité des résidus.
En conclusion, ce test permet de dire que les résidus semblent suivre une loi normale








